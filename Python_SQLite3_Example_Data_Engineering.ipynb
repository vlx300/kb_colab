{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python SQLite3 Example- Data Engineering.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vlx300/kb_colab/blob/master/Python_SQLite3_Example_Data_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaNy2ohpkkNJ",
        "colab_type": "text"
      },
      "source": [
        "#**Data Engineering** \n",
        "#**Relational vs Non-Relational Databases**\n",
        "\n",
        "![alt text](https://ulriklyngs.com/sites/default/files/styles/final-blog-image-style/public/desat_chalkb.jpg?itok=7gumzPT8)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4-TmumQcgor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sqlite3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWdkAo6ScnS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "db = sqlite3.connect(':memory:')  # using in memory database\n",
        "cur = db.cursor()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgeMBDs1gZla",
        "colab_type": "text"
      },
      "source": [
        "##**Create three tables**\n",
        "\n",
        "1.   **Customer**:  This table contains primary key, as well as customer first and last names\n",
        "2.   **Items**: this table will contain the primary key, item name and item price\n",
        "3.   **Items Bought**:this table contains the order#, date and price, it will also connect to the primary keys in items and customer tables. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjhMW3yLc0sC",
        "colab_type": "code",
        "outputId": "9dd74005-8a90-4147-9afa-8dffa0116076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''CREATE TABLE IF NOT EXISTS customer (\n",
        "  id integer PRIMARY KEY,\n",
        "  firstname varchar(255),\n",
        "  lastname varchar(255) )''')\n",
        "cur.execute('''CREATE TABLE IF NOT EXISTS Item (\n",
        "  id integer PRIMARy KEY,\n",
        "  title varchar(255),\n",
        "  price decimal )''')\n",
        "cur.execute('''CREATE TABLE IF NOT EXISTS BoughtItem (\n",
        "  ordernumber integer PRIMARY KEY,\n",
        "  customerid integer,\n",
        "  itemid integer, \n",
        "  price decimal,\n",
        "  CONSTRAINT customerid\n",
        "      FOREIGN KEY (customerid) REFERENCES Customer(id),\n",
        "  CONSTRAINT itemid \n",
        "      FOREIGN KEY (itemid) REFERENCES Item(id) )''')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7f5358307dc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7aEfEwqhunN",
        "colab_type": "text"
      },
      "source": [
        "You passed a query to cur.execute() to create your three tables.. Now lets populate them with data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkGEx3-gfgMR",
        "colab_type": "code",
        "outputId": "be622fe2-82a4-43f0-9d11-02af30329450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''INSERT INTO Customer(firstname, lastname)\n",
        "                VALUES ('Bob', 'Adams'),\n",
        "                ('Amy', 'Smith'),\n",
        "                ('Rob', 'Bennet');''')\n",
        "cur.execute('''INSERT INTO Item(title, price)\n",
        "                VALUES ('USB', 10.2),\n",
        "                ('Mouse', 12.23),\n",
        "                ('Monitor', 199.99);''')\n",
        "cur.execute('''INSERT INTO BoughtItem(customerid, itemid, price)\n",
        "                VALUES (1, 1, 10.2),\n",
        "                (1, 2, 12.23),\n",
        "                (1, 3, 199.99),\n",
        "                (2, 3, 180.00),\n",
        "                (3, 2, 11.23);''')  # Discounted Price\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sqlite3.Cursor at 0x7f5358307dc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSQjQndtkDhB",
        "colab_type": "text"
      },
      "source": [
        "Now we have a few records in each table, you can use this data to answer a few more questions \n",
        "\n",
        "#**SQL Aggregation Functions**\n",
        "\n",
        "Aggregation functions are those that perform mathematical operations  on a result set.  **AVG, COUNT, MIN, MAX and SUM**  Often you will need a **GROUP BY** or **HAVING** Clause to complement these aggregations. Let use **AVG** as a example: (See Below)\n",
        "\n",
        "*AVG can compute the \"mean\" of a given resul*t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0RSmg27kBZT",
        "colab_type": "code",
        "outputId": "cc10bbf8-b47b-4960-a3bd-6ded7269dfc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''SELECT itemid, AVG(price) FROM Boughtitem GROUP BY itemid''')\n",
        "print(cur.fetchall())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(1, 10.2), (2, 11.73), (3, 189.995)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW9Qhegoo6Go",
        "colab_type": "text"
      },
      "source": [
        "Here you have retrieved the averge price for each of the items bought in your database. you can see that the item with the item id# of 1 has an average price of $10.20\n",
        "\n",
        "Lets make this easier to understand, by displaying the item name instead of item id#"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7W9mVPRZogXX",
        "colab_type": "code",
        "outputId": "3c499ea6-ce26-4ea6-9558-9ce726583a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''SELECT item.title, AVG(boughtitem.price) FROM Boughtitem as boughtitem\n",
        "            INNER JOIN Item as Item on (item.id = boughtitem.customerid)\n",
        "            GROUP BY boughtitem.itemid''')\n",
        "\n",
        "print(cur.fetchall())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('USB', 10.2), ('Monitor', 11.73), ('Mouse', 189.995)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4jluSfrG9n",
        "colab_type": "text"
      },
      "source": [
        "Another useful aggregation function is **SUM**. you can use this functon to display the total amount of money each customer spent (*See below*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgiPOsV7qcOi",
        "colab_type": "code",
        "outputId": "509e14b0-04d1-4307-fa33-e6c444d3b94e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''SELECT customer.firstname, SUM(boughtitem.price) FROM BoughtItem as BoughtItem\n",
        "            INNER JOIN Customer as Customer on (Customer.id = boughtitem.customerid)\n",
        "            GROUP BY customer.firstname''')\n",
        "\n",
        "print(cur.fetchall())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('Amy', 180), ('Bob', 222.42000000000002), ('Rob', 11.23)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxtzSfPvt9oc",
        "colab_type": "text"
      },
      "source": [
        "#**Speeding up SQL Queries**\n",
        "\n",
        "Speed depends on various factors but is mostly affected by how many of each of the following are present: \n",
        "\n",
        "*   **Joins**\n",
        "*   **Aggregations**\n",
        "*   **Traversals**\n",
        "*   **Records**\n",
        "\n",
        "the greater number of joins, the higher the complexity and the larger number of traversals in tables. Multiple joins are quite expensive to performs on several thousand records invloving several tables because *the database needs to cache the intermediate result*! At this point most people starting thinking about increasing DB memory size. Hmmm\n",
        "\n",
        "![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRc8ZoZdP57kkw--eYUpsNhp-DnJOs7fblaUIgnFd4uMORRB9sATg&s)\n",
        "\n",
        "Speed is also affected by whether or not there are indices present in the database or not. they are extrememly important and allow you to quick;ly searh thru a table to find a match for some column etc.  \n",
        "\n",
        "Indices sort the records at th4 cost of higher insert time. as well as some storage Multiple columns can be combined to create a single index.  *Example. 'date' and 'price'columns can be combined because the query depends on both conditions.* \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Jo12O26C_kw",
        "colab_type": "text"
      },
      "source": [
        "#**Debugging SQL queries**\n",
        "\n",
        "Most dstabases include an EXPLAIN QUERY PLAN that describes the steps the database takes to execute a query. for SQLite you can enable this functionality by adding EXPLAIN QUERY PLAN  in front of a SELECT statement "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XUGx-cODzIA",
        "colab_type": "code",
        "outputId": "2302dc13-318c-4e79-f429-83970203fde8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cur.execute('''EXPLAIN QUERY PLAN SELECT customer.firstname, item.title,\n",
        "                item.price, boughtitem.price FROM BoughtItem as boughtitem\n",
        "                INNER JOIN Customer as customer on (customer.id = boughtitem.customerid)\n",
        "                INNER JOIN  Item as item on (item.id = boughtitem.itemid)''')\n",
        "\n",
        "print(cur.fetchall())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0, 0, 'SCAN TABLE BoughtItem AS boughtitem'), (0, 1, 1, 'SEARCH TABLE Customer AS customer USING INTEGER PRIMARY KEY (rowid=?)'), (0, 2, 2, 'SEARCH TABLE Item AS item USING INTEGER PRIMARY KEY (rowid=?)')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybETnO_EFBcP",
        "colab_type": "text"
      },
      "source": [
        "the query tries to list the first name, item title, original price for all bought items. \n",
        "\n",
        "**SQL Query Plan**\n",
        "\n",
        "SCAN TABLE BoughtItem AS boughtitem\n",
        "SEARCH TABLE Customer AS customer USING INTEGER PRIMARY KEY (rowid=?)\n",
        "SEARCH TABLE Item AS item USING INTEGER PRIMARY KEY (rowid=?)\n",
        "\n",
        "**NOTE:** the fetch statement in the python code only returns the explanation, **NOT THE RESULTS!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T43xeYEGWh0",
        "colab_type": "text"
      },
      "source": [
        "#**Non-Relational databases** (Exp...NoSQL)\n",
        "\n",
        "In the above section,  we laid out the differences between relational vs non-relational databases an used SQLite with python .  Now lets focus on NoSQL. We will use Mongo DB as example and use the same data as above to see the differences (*Laptop wont allow my connections thru FW, so we will just discuss*)\n",
        "\n",
        "**Mongo DB is a document based No-SQL database and is Very Scalable**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPdL7tqJGsfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pymongo  # mongo Db comes pre-installed in Google COLAB\n",
        "\n",
        "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
        "# NOTE: this database wont be created until it is populated with data\n",
        "\n",
        "db = client[\"example_database\"]\n",
        "\n",
        "customers = db[\"customers\"]\n",
        "items = db[\"items\"]\n",
        "\n",
        "customers_data = [{ \"firstname\": \"Bob\", \"lastname\": \"Adams\"},\n",
        "                  { \"firstname\": \"Amy\", \"lastname\": \"Smith\"},\n",
        "                  { \"firstname\": \"Rob\", \"lastname\": \"Bennet\"},]\n",
        "items_data = [{ \"title\": \"USB\", \"price\": 10.2},\n",
        "              { \"title\": \"Mouse\", \"price\": 12.23},\n",
        "              { \"title\": \"Monitor\", \"price\": 199.99},]\n",
        "\n",
        "customers.insert_many(customers_data)\n",
        "items.insert_many(items_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de6vGVuDaUu",
        "colab_type": "text"
      },
      "source": [
        "#**NoSQL vs SQL**\n",
        "\n",
        "Long term storage solutions\n",
        "\n",
        "If you have a constantly changing schema,  such as finanncial regulatory information (GLBA SOX etc.)then No SQL can modify the records, and nest related information . Juat imagine the number of joins you would need to do in SQL if you had Eight orders of nesting.  Now if your goal is to \"run reports\" and extract information form the financial data, and ife conclusions? - in this case you would need to run complex queries which SQL tends to be faster doing.  so depends on what you need to get done. \n",
        "\n",
        "NOTE: SQL databases (PostgreSQL) has relases a feature that allows \"querable JSON data\" to be inserted as part of a record. Speed may be a concerns with this \n",
        "\n",
        "**Note: it is faster to query unstructured data from no-SQL Db than it is to query JSON fields from a JSON type column in PostgreSQL**. \n",
        "\n",
        "Speed isnt the only metric that matters. You also need to look at things such as **Transcations, Atomicity, durability anmd scalability**.  Transactions are important for financial applications.\n",
        "\n",
        "\n",
        "\n",
        "*   **Elastic Search**: highly efficient in Text searches. Document based database to crate a powerful searh tool.\n",
        "*   **Newt DB**: combines ZODB and PostgreSQL JSON feature to create a Python friendly No-SQL DB\n",
        "*   **Influx DB** : time seried Db to store events. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9EGfHXYVnRP",
        "colab_type": "text"
      },
      "source": [
        "#**Cache databases**\n",
        "\n",
        "a cache database is a very fast, short term storage solution for shorth lived structed and unstructured data. It can be partinioned and scaled according to you specific needs and requirements. typically smaller in size than your main DB and becuase of this your cache databse normally resides in memory bypassoing the need to read from disk. \n",
        "\n",
        "the normally live alongside your main SQL and No-SQL databases with the goal of alleviate load and speed response times\n",
        "###**REDIS  Example**\n",
        "\n",
        "If you ve ever used python dictionaries, then REDIS follows the same structure. Is a key-value store where you can SET and GET just like a python DICT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmBu6CuxohUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install redis"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPgXGOy7oqdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import redis\n",
        "from datetime import timedelta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vEm2DtXor3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# in a real web application, configuration is obtained via settings or utils \n",
        "r = redis.Redis()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFqUoCHOpJnb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Assume this is a getter handling a request \n",
        "def get_name(request, *args, **kwargs):\n",
        "  id = request.get('id')\n",
        "  if id in r:\n",
        "    return r.get(id)  # Assume we have an {id: name} store\n",
        "  else:\n",
        "    # get some data from the main DB here, we assume we already did it\n",
        "    name = 'Bob'\n",
        "    # set the valule in the cache database, with expiration time\n",
        "    r.setex(id, timedelta(minutes=60), value=name)\n",
        "    return name"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qvqe7wm4rFXW",
        "colab_type": "text"
      },
      "source": [
        "#**Design Patterns and ETL Concepts**\n",
        "\n",
        "In larfge applications, you will oftern use mor ethan on type of database. I fact, its possibloe to utilize PostgreSQL, MongoDB and Redis all within one application. One challenging problem is state changes betweeen databases. this exposes the developer to issue of consistency . Conside rthe below example:\n",
        "\n",
        "1. A value in database #1 is updated\n",
        "2. that same value in database #2. Is NOT updated. \n",
        "3. A query is run on database #2\n",
        "\n",
        "Now we have outdated results. The result returned from databse #2 wont reflect the updated data from database #1. This can happend with any two databases. **This is especially common is the main database is NoSQL**. Exp MongoDB. and the data is bring transformed into a SQL database (sqlite) for query purposes.\n",
        "\n",
        "Databses have \"backroun-workers\" to tackle such issues.   These workers **EXTRACT** data from one database, **TRANSFORM** it in some way and then **LOAD** it into the target database.   When converting froim the NoSQL databse to SQL database, the ETL process takes th following steps:\n",
        "\n",
        "1. **EXTRACT**:  there is a MONGO Db trigger whenever a record is created/updated and so on. A callback functioned is called \"Asynchronously\" on a seprate thread. \n",
        "\n",
        "2. **TRANSFORM**: Parts of the record are extracted, normalized, and put into the correct data structure \"Row\"to be inserted into SQL.\n",
        "\n",
        "3. **LOAD**: the SQL database is updated in batches or as a single record for High-Volume writes\n",
        "\n",
        "this workflow is quite common for financial, gaming and reporting applications. In these cases, the constantly changing schema requires a NoSQL database, but reporting, analysis and aggregation requires a SQL database. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLxip6tVOt9y",
        "colab_type": "text"
      },
      "source": [
        "#**ETL Challenges**\n",
        "\n",
        "There are several challenging concepts in ETL:\n",
        "\n",
        "1. **Big-Data**\n",
        "2. **Stateful problems**\n",
        "3. **Asynchronous Worker**s\n",
        "4. **Type-Matching**\n",
        "\n",
        "and the list goes on. However since the steps in ETL proxess are well-defined and logical, the data and backend engineers will typically worry more about performance and availability rather than implementation. \n",
        "\n",
        "If you application is writing thousands of records per second to MongoDB, then your ETL worker neds to keep up with transforming, loading nd delivering the data to the user in  requested form. Speed and latency can become an issue.   Typically these worker will be written in fast languages. You can use compiled code for the Transform step.you may want to consider. \n",
        "\n",
        "**NOTE**:  Multi-processing and separation of workers are other solutions \n",
        "Consider Utilizing Numba (http://numba.pydata.org/) for CPU intensive functions.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhqUFUSqSpAW",
        "colab_type": "text"
      },
      "source": [
        "#**Design Patterns in Big-Data**\n",
        "\n",
        "when you have terabytes of data, you will need mutiple machine sot handle all of that data. a database aggregation funciton can be a very complex operation. How can you query, aggregate and make use of relatively big data in a efficient way?\n",
        "\n",
        "##**Apache Map-Reduce**\n",
        "Introduced by apache foundation,  this follow the Map, Shuffle, Reduce workflow.  the idea is to map different data on separate machines (**Clusters**), then you can perform work on the data, grouped by a key, and finally aggregate thr data at the final stage. This workflow is still in use today, but has been fading recently in favor of **Apache Spark**  https://spark.apache.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Csx9xdEdMyOx",
        "colab_type": "text"
      },
      "source": [
        "##**Common Aspects of ETL process and Big Data workflows**\n",
        "\n",
        "Both workflows ollow the Producer-Consumer pattern. A worker (The Producer) produces data of some  kind. and outputs to the pipeline. this pipeline can take many forms including networ messages and triggers. After the producer outputs the data , the consumer consumes and make use of the data.  these worker typically work in an asynchronous manne and executed via seperate proceses. \n",
        "\n",
        "the producer is like the **Extract** and **Transform** Steps in the above ETL process. Similarly in Big Data, the Mapper can be seen as the Producer, and the Reducer is effectively the consumer. This seperation of concerns is extremely important in the development and architecure design of applications  "
      ]
    }
  ]
}